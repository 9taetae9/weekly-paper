
## 🇶 HashSet의 내부 동작 방식과 중복 제거 메커니즘을 설명하고, HashSet이 효율적인 중복 체크를 할 수 있는 이유를 설명해주세요.
### HashSet
- set 인터페이스로 구현할 컬렉션
- 내부적으로 HashMap으로 구현
- <key,value>로 저장 key가 고유하므로 중복저장 되지 않는다.
- 저장 순서와 출력 순서를 보장하지 않는다
  #### HashCode
  - 기본적으로 객체의 메모리 번지 이용하여 객체의 고유한 값을 정수로 만들어 반환
  - 필요에 따라 오버라이딩 가능
  - 객체 별로 구분할 수 있는 고유한 값을 만들어 준다.
  #### 버킷 수
  - defalut 버킷 size=16, hashcode를 버킷 수로 나눈 나머지를 hash 버킷 index로 사용
  - 로드팩터
    - HashMap을 default 로드팩터는 0.75
    - 0.75이상 -> 버킷 개수의 2배만큼 용량 증가 %16 -> %32
  #### 중복 메커니즘
  1. 객체를 저장하기 전에 hashCode()호출
  2. 해시 코드를 얻은 다음 저장되어 있는 객체들의 해시코드와 비교
  3. 해시코드가 같다면 equals()메소드로 비교 true면 동일 객체로 판단 후 중복 저장 x\
  #### 결론
  - HashSet은 해시 함수를 사용하여 특정 값에 대한 고유의 다이제스트를 얻기 때문에 효율적으로 중복체크 가능 O(1)로 해결
## 🇶 O(n)과 O(log n)의 성능 차이를 실생활 예시를 들어 설명하고, 데이터의 크기가 1백만 개일 때 각각 대략 몇 번의 연산이 필요한지 비교해주세요.
### O(n) (선능 시간 복잡도)
- 선형 시간 복잡도인 O(n) 은 데이터 크기 n에 비례하여 연산 횟수가 증가하는 경우입니다. 즉, 데이터가 하나씩 늘어날 때마다 수행해야 하는 작업도 동일하게 늘어난다.
- 예시
  -  길게 늘어선 대기열에서 사람 찾기: 대기열에서 특정 사람을 찾으려면 첫 번째 사람부터 끝까지 하나씩 확인
### O(log n) (로그 시간 복잡도)
- **로그 시간 복잡도**인 O(log n)은 데이터 크기 n이 증가할 때 연산 횟수의 증가가 매우 느린 경우입니다. 데이터의 크기가 두 배가 되어도 연산 횟수는 일정 비율로 증가하는 특성이 있습니다. 이는 데이터가 **정렬된 상태**에서 이진 탐색처럼 중간값을 비교하며 문제를 절반씩 줄여나가는 방식
- 예시
  -  이진 탐색 알고리즘: 리스트가 정렬된 상태에서 원하는 값을 찾을 때, 중간값을 기준으로 왼쪽 또는 오른쪽 절반을 선택하여 계속해서 범위를 좁혀가며 찾습니다.
### O(n)과 O(log n) 성능 차이
- O(n)은 데이터가 늘어날 때마다 연산 횟수가 선형적으로 증가합니다. 1백만 개의 데이터에서 O(n)은 1백만 번의 연산을 필요로 합니다.
- O(log n)은 데이터 크기가 두 배가 되어도 연산 횟수는 고작 1번 정도만 증가합니다. 1백만 개의 데이터에서 O(log n)은 약 20번의 연산으로 끝납니다.

| **알고리즘** | **데이터 크기 1백만 개일 때 연산 횟수** |
| --- | --- |
| **O(n)** | 1,000,000 번 |
| **O(log n)** | 약 20 번 |

### 결론
- O(n)은 데이터 크기 증가에 따라 연산 횟수가 비례적으로 증가하여, 1백만 개 데이터에서는 1,000,000번의 연산을 수행합니다.
- O(log n)은 데이터 크기 증가에 따른 연산 횟수 증가가 매우 느리기 때문에, 1백만 개 데이터에서는 약 20번의 연산으로 해결할 수 있습니다.
> 따라서 O(log n) 알고리즘은 데이터가 매우 많을 때도 매우 효율적으로 동작하며, 성능이 O(n)에 비해 훨씬 뛰어납니다.
